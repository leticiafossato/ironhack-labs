{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MLBUDdX8SEjl","colab_type":"text"},"source":["# Parallelization Lab\n","\n","In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab.\n","\n","### Step 1: Use the requests library to retrieve the content from the URL below."]},{"cell_type":"code","metadata":{"id":"HlQZvSUqSEjn","colab_type":"code","colab":{}},"source":["import requests\n","\n","url = 'https://en.wikipedia.org/wiki/Data_science'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHlgfatrSEjs","colab_type":"code","colab":{}},"source":["# your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ig5G0TTDSEjw","colab_type":"text"},"source":["### Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."]},{"cell_type":"code","metadata":{"id":"zahpf2s-SEjx","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VfmrZsypSEj0","colab_type":"code","colab":{}},"source":["# your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7TXSNXCSEj3","colab_type":"text"},"source":["### Step 3: Use list comprehensions with conditions to clean the link list.\n","\n","There are two types of links, absolute and relative. Absolute links have the full URL and begin with *http* while relative links begin with a forward slash (/) and point to an internal page within the *wikipedia.org* domain. Clean the respective types of URLs as follows.\n","\n","- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n","- Relative Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n","- Combine the list of absolute and relative links and ensure there are no duplicates."]},{"cell_type":"code","metadata":{"id":"1ZqaDQ4gSEj4","colab_type":"code","colab":{}},"source":["domain = 'http://wikipedia.org'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHDkpTrISEj7","colab_type":"code","colab":{}},"source":["# your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"liPZ-FYLSEkA","colab_type":"text"},"source":["### Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."]},{"cell_type":"code","metadata":{"id":"S5I-1EAcSEkA","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jErdx2cdSEkD","colab_type":"code","colab":{}},"source":["# your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyDm2n-rSEkG","colab_type":"text"},"source":["### Step 5: Write a function called index_page that accepts a link and does the following.\n","\n","- Tries to request the content of the page referenced by that link.\n","- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n","    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip3 install python-slugify`.\n","    - To import the slugify function, you would do the following: `from slugify import slugify`.\n","    - You can then slugify a link as follows `slugify(link)`.\n","- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n","- If an exception occurs during the process above, just `pass`."]},{"cell_type":"code","metadata":{"id":"cTzUHCpBSEkH","colab_type":"code","colab":{}},"source":["from slugify import slugify"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mm7-pzw4SEkK","colab_type":"code","colab":{}},"source":["# your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVDNEdyVSEkM","colab_type":"text"},"source":["### Step 6: Sequentially loop through the list of links, running the index_page function each time.\n","\n","Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run. \n","\n","_hint: Use tqdm to keep track of the time._ "]},{"cell_type":"code","metadata":{"id":"1akpUc5_SEkN","colab_type":"code","colab":{}},"source":["# your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KWa8qXs3SEkQ","colab_type":"text"},"source":["### Step 7: Perform the page indexing in parallel and note the difference in performance.\n","\n","Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run.\n","\n","Use both methods, i.e., for one hand use the `multiprocess` module to use the function created in the jupyter notebook and run the download in parallel.\n","\n","And for another hand create a python file containing the function to download the file and use the `multiprocessing` module to run. "]},{"cell_type":"code","metadata":{"id":"BSA-oQV9SEkT","colab_type":"code","colab":{}},"source":["# your code here\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Sx1hLdbTEp5","colab_type":"text"},"source":["**BONUS**: Create a function that counts how many files are there in the wikipedia folder using the `os` module. \n","\n","Delete the files from the folder before you run and perform the above solution asynchronously. \n","\n","Use your function to check how many files are being downloaded."]}]}