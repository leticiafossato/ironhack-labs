{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:16:00.091692Z",
     "start_time": "2020-09-06T01:16:00.087691Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Web Scraping Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a sequence of steps used to obtain some information from the web. Try to understand the code and then refactor it into a pipeline-like structure. This is not a simple task and there are hundreds of ways to do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract links for the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:06:03.584117Z",
     "start_time": "2020-09-06T01:05:54.053506Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.opendota.com/matches/pro'\n",
    "\n",
    "# we need to use selenium since the page loads dynamic html\n",
    "driver = webdriver.Chrome(executable_path='C:/Users/andreaguiar/Desktop/usr/dist/chromedriver.exe')\n",
    "\n",
    "#navigate to page\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# find all table rows\n",
    "rows = driver.find_elements_by_tag_name('tr')\n",
    "\n",
    "# get first column [0] of each row (skip first row)\n",
    "row_data = [row.find_element_by_tag_name('td') for row in rows[1::]]\n",
    "\n",
    "# this code extracts the links from tag a's\n",
    "links = [item.find_element_by_tag_name('a').get_attribute('href') for item in row_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:06:03.591093Z",
     "start_time": "2020-09-06T01:06:03.585094Z"
    }
   },
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter links to get info\n",
    "\n",
    "### Let's first create the code for 1 link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:06:21.696702Z",
     "start_time": "2020-09-06T01:06:21.693731Z"
    }
   },
   "outputs": [],
   "source": [
    "# get each link\n",
    "inside_url = links[0] \n",
    "print(inside_url)\n",
    "\n",
    "# get only the numerical part of the link, which is the match id\n",
    "match_id = re.findall('\\d+', inside_url)[0]\n",
    "print(match_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:06:48.491589Z",
     "start_time": "2020-09-06T01:06:46.234116Z"
    }
   },
   "outputs": [],
   "source": [
    "# navigate to page\n",
    "driver.get(inside_url)\n",
    "\n",
    "# we need to give some time for the browser to reach the page.\n",
    "time.sleep(2)\n",
    "\n",
    "# get tables\n",
    "table1 = driver.find_elements_by_tag_name('table')[0]\n",
    "table2 = driver.find_elements_by_tag_name('table')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download table1 to a dataframe and start its cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:06:55.078496Z",
     "start_time": "2020-09-06T01:06:52.685859Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get table1 (index=0)\n",
    "df1 = pd.read_html(driver.page_source)[0]\n",
    "\n",
    "# throw away some columns and rows (you know there should only be 5 rows == 5 players for each team)\n",
    "df1 = df1.drop(columns={'/','/.1','Jogador', 'Buffs'}).rename(columns={'Unnamed: 16':'jungle_itens'})\n",
    "df1 = df1.loc[:4,:]\n",
    "\n",
    "# rename the columns so as to obtain a readable dataframe\n",
    "df1 = df1.rename(columns={'K':'kills','M':'deaths','A':'assistences','LVL':'lvl','FN':'creep_kill','N':'creep_deny', 'OPM':'gold_per_minute', \n",
    "                        'XPM':'exp_per_minute', 'Itens':'itens','O':'total_gold','DH':'hero_damage','DT':'tower_damage','CH':'hero_heal', 'Buffs':'tomes'})\n",
    "\n",
    "# get names, links and ids of each user\n",
    "\n",
    "names1 = [name.text for name in table1.find_elements_by_tag_name('a')]\n",
    "name_links1 = [name.get_attribute('href') for name in table1.find_elements_by_tag_name('a')]\n",
    "user_ids1 = [re.findall('\\d+', link)[0] for link in name_links1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean item names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code finds the name of the itens the player has bought. Don't worry about the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:03.437357Z",
     "start_time": "2020-09-06T01:07:01.686985Z"
    }
   },
   "outputs": [],
   "source": [
    "itens1 = [name.find_element_by_class_name('sc-uJMKN') \n",
    "             for name in table1.find_elements_by_tag_name('td')\n",
    "                     if len(name.find_elements_by_class_name('sc-uJMKN')) > 0]\n",
    "\n",
    "name_itens1 = [re.findall('items/(.*)_lg\\.png',item.find_element_by_tag_name('object').get_attribute('data'))[0] \n",
    "             for item in itens1]\n",
    "\n",
    "# Clean jungle item names\n",
    "jungle_itens1 = [name.find_element_by_class_name('neutral') \n",
    "             for name in table1.find_elements_by_tag_name('td') \n",
    "                     if len(name.find_elements_by_class_name('neutral')) > 0]\n",
    "\n",
    "name_jungle_itens1 = [re.findall('items/(.*)_lg\\.png',item.find_element_by_tag_name('object').get_attribute('data'))[0] \n",
    "             for item in jungle_itens1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input values into table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:05.411442Z",
     "start_time": "2020-09-06T01:07:05.405413Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['jungle_itens'] = name_jungle_itens1\n",
    "df1['itens'] = name_itens1\n",
    "df1['name'] = names1\n",
    "df1['links'] = name_links1\n",
    "df1['user_ids'] = user_ids1\n",
    "df1['match_id'] = 5 * [match_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is your final dataframe for team 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:10.212844Z",
     "start_time": "2020-09-06T01:07:10.197814Z"
    }
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's repeat the process for team 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:15.251598Z",
     "start_time": "2020-09-06T01:07:13.273516Z"
    }
   },
   "outputs": [],
   "source": [
    "# get table1 (index=1)\n",
    "df2 = pd.read_html(driver.page_source)[1]\n",
    "\n",
    "df2 = df2.drop(columns={'/','/.1','Jogador', 'Buffs'}).rename(columns={'Unnamed: 16':'jungle_itens'})\n",
    "df2 = df2.loc[:4,:]\n",
    "\n",
    "df2 = df2.rename(columns={'K':'kills','M':'deaths','A':'assistences','LVL':'lvl','FN':'creep_kill','N':'creep_deny', 'OPM':'gold_per_minute', \n",
    "                        'XPM':'exp_per_minute', 'Itens':'itens','O':'total_gold','DH':'hero_damage','DT':'tower_damage','CH':'hero_heal', 'Buffs':'tomes'})\n",
    "\n",
    "names2 = [name.text for name in table2.find_elements_by_tag_name('a')]\n",
    "name_links2 = [name.get_attribute('href') for name in table2.find_elements_by_tag_name('a')]\n",
    "user_ids2 = list(map(lambda x : re.findall('\\d+', x)[0], name_links2))\n",
    "\n",
    "itens2 = [name.find_element_by_class_name('sc-uJMKN') \n",
    "             for name in table2.find_elements_by_tag_name('td')\n",
    "                     if len(name.find_elements_by_class_name('sc-uJMKN')) > 0]\n",
    "\n",
    "name_itens2 = [re.findall('items/(.*)_lg\\.png',item.find_element_by_tag_name('object').get_attribute('data'))[0] \n",
    "             for item in itens2]\n",
    "\n",
    "\n",
    "jungle_itens2 = [name.find_element_by_class_name('neutral') \n",
    "             for name in table1.find_elements_by_tag_name('td') \n",
    "                     if len(name.find_elements_by_class_name('neutral')) > 0]\n",
    "\n",
    "name_jungle_itens2 = [re.findall('items/(.*)_lg\\.png',item.find_element_by_tag_name('object').get_attribute('data'))[0] \n",
    "             for item in jungle_itens2]\n",
    "\n",
    "df2['jungle_itens'] = name_jungle_itens2\n",
    "df2['itens'] = name_itens2\n",
    "df2['name'] = names2\n",
    "df2['links'] = name_links2\n",
    "df2['user_ids'] = user_ids2\n",
    "df2['match_id'] = 5 * [match_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:15.267614Z",
     "start_time": "2020-09-06T01:07:15.252581Z"
    }
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First let's clean up the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code was written in a rush. Let's clean up the mess of df1 and table1 stuff and create a function. ** \n",
    "\n",
    "Create a function that receives `table` like so:\n",
    "\n",
    "```python\n",
    "def clean_table(table, index=0):\n",
    "    # organize the processess above ...\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "This function will receive the table object from the selenium driver element and it should return a cleaned dataframe with the above processes. If you specify `index=0`, it should return the table for the first team, if you specify `index=1` it should return the table for the second team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:33.129519Z",
     "start_time": "2020-09-06T01:07:33.119519Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_table(table, index=0):\n",
    "    \n",
    "    df = pd.read_html(driver.page_source)[index].drop(columns={'/','/.1','Jogador', 'Buffs'}).rename(columns={'Unnamed: 16':'jungle_itens'})\n",
    "    df = df.loc[:4,:]\n",
    "    \n",
    "    df = df.rename(columns={'K':'kills','M':'deaths','A':'assistences','LVL':'lvl','FN':'creep_kill','N':'creep_deny', 'OPM':'gold_per_minute', \n",
    "                            'XPM':'exp_per_minute', 'Itens':'itens','O':'total_gold','DH':'hero_damage','DT':'tower_damage','CH':'hero_heal', 'Buffs':'tomes'})\n",
    "    names = [name.text for name in table.find_elements_by_tag_name('a')]\n",
    "    name_links = [name.get_attribute('href') for name in table.find_elements_by_tag_name('a')]\n",
    "    user_ids = list(map(lambda x : re.findall('\\d+', x)[0], name_links))\n",
    "    match_id = match_id = re.findall('\\d+', driver.current_url)[0]\n",
    "    \n",
    "    # get item names\n",
    "    itens = [name.find_element_by_class_name('sc-uJMKN') \n",
    "             for name in table.find_elements_by_tag_name('td')\n",
    "                     if len(name.find_elements_by_class_name('sc-uJMKN')) > 0]\n",
    "\n",
    "    name_itens = ([re.findall('items/(.*)_lg\\.png',item.find_element_by_tag_name('object').get_attribute('data'))[0] \n",
    "                     for item in itens] + 5 * [''])[:5]\n",
    "\n",
    "    jungle_itens = [name.find_element_by_class_name('neutral') \n",
    "                         for name in table.find_elements_by_tag_name('td') \n",
    "                                 if len(name.find_elements_by_class_name('neutral')) > 0]\n",
    "    \n",
    "    \n",
    "    name_jungle_itens = ([re.findall('items/(.*)_lg\\.png',item.find_element_by_tag_name('object').get_attribute('data'))[0] \n",
    "                                 for item in jungle_itens] + 5 * [''])[:5]\n",
    "    \n",
    "    df['jungle_itens'] = name_jungle_itens\n",
    "    df['itens'] = name_itens\n",
    "    df['name'] = names\n",
    "    df['links'] = name_links\n",
    "    df['user_ids'] = user_ids\n",
    "    df['match_id'] = 5 * [match_id]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In such a way that the following code returns the dataframe of the first team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:39.409500Z",
     "start_time": "2020-09-06T01:07:35.479942Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "table = driver.find_elements_by_tag_name('table')[0]\n",
    "team_a = clean_table(table, index=0)\n",
    "\n",
    "# and this one receives the second team\n",
    "table = driver.find_elements_by_tag_name('table')[1]\n",
    "team_b = clean_table(table, index=1)\n",
    "\n",
    "teams = pd.concat([team_a, team_b]).reset_index(drop=True)\n",
    "\n",
    "teams['team'] = ['a']*5 + ['b']*5\n",
    "teams['date'] = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:07:41.931254Z",
     "start_time": "2020-09-06T01:07:41.913254Z"
    }
   },
   "outputs": [],
   "source": [
    "teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store in database\n",
    "\n",
    "Create the `dota` database on your pgAdmin and substitute `admin` by your own password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:08:12.012928Z",
     "start_time": "2020-09-06T01:08:11.674266Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://postgres:admin@localhost/dota')\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get all the tables from the page and append them in a table in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:08:13.362320Z",
     "start_time": "2020-09-06T01:08:13.360319Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger('dota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:08:14.181059Z",
     "start_time": "2020-09-06T01:08:13.890473Z"
    }
   },
   "outputs": [],
   "source": [
    "teams.to_sql('pro_matches', conn, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:08:14.556838Z",
     "start_time": "2020-09-06T01:08:14.543839Z"
    }
   },
   "outputs": [],
   "source": [
    "team_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T01:12:29.752519Z",
     "start_time": "2020-09-06T01:12:11.161681Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## df = pd.DataFrame()\n",
    "engine.execute('DROP TABLE IF EXISTS pro_matches')\n",
    "\n",
    "for link in tqdm(links):\n",
    "    # navigate to link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # give time for browser to reach page\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    print(link)\n",
    "    # Add check to only perform the code below if the value is not in the database (add this condition on the pipeline)\n",
    "    match_id = re.findall('\\d+', link)[0]\n",
    "    \n",
    "    \n",
    "    # if the match_id was not found in the database:\n",
    "    logger.info(f'Match {match_id} not yet found in database. Storing it.')\n",
    "\n",
    "    try:\n",
    "        table = driver.find_elements_by_tag_name('table')[0]\n",
    "        team_a = clean_table(table)\n",
    "\n",
    "        table = driver.find_elements_by_tag_name('table')[1]\n",
    "        team_b = clean_table(table)\n",
    "\n",
    "        teams = pd.concat([team_a, team_b])\n",
    "        logger.info('Appending results')\n",
    "\n",
    "        # remove '-' from integer dtypes and convert to string\n",
    "        teams = teams.convert_dtypes().applymap(lambda x : str(x).replace('-','0')).astype(str)\n",
    "        teams.to_sql('pro_matches', conn, if_exists='append', index=False, )\n",
    "\n",
    "\n",
    "        df = pd.concat([df, teams])\n",
    "\n",
    "\n",
    "    except:\n",
    "        logger.warning(f'{link} could not be stored.')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missions: reestructure this process into a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>COOKIECUTTER</u>: Use cookiecutter to create your new structure of files\n",
    "\n",
    "After installing cookiecutter, run:\n",
    "`cookiecutter https://github.com/aguiarandre/etl-pipelines`\n",
    "\n",
    "This will create your pipeline's folder structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>ORGANIZATION - USING .PY FILES</u>: Transform the above steps into a structured .py pipeline\n",
    "\n",
    "Remember to separate the parameters on their own separate file. The connection on another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>DOCUMENTATION</u>: Document each function of your pipeline. Then use sphinx to create your code's documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Go into the `your_project/docs` folder and `./make.bat html` or `./make html` (don't forget to run `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>IDEMPOTENCY PRINCIPLE</u>: Avoid duplication in your database. Only perform the storage step if today's date is not there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: <u>MORE BENEFITS OF .PY FILES</u>: Create a scheduler to perform this task once a day.\n",
    "\n",
    "Use **crontab** if you're a Mac user.\n",
    "After allowing cron to have Full Disk Access on `Security & Privacy`, write in your crontab: \n",
    "\n",
    "> `* * * * * full/path/to/your/python/executable full/path/to/pipeline.py`\n",
    "\n",
    "Use **task-scheduler** if you are on windows. Create a `run.bat` script on the same folder of your `pipeline.py`. Write inside: \n",
    "\n",
    "> `python.exe pipeline.py`\n",
    "\n",
    "Then go to task-scheduler (Agendar Tarefas) and create a new task. Give it a name, a new trigger specifying times and a new action specifying the path/to/your/run.bat and fill in 'Start at' with the path to your /project/src folder where your pipeline lives in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
